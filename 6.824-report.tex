\documentclass[10pt]{article}

\input{preamble} \input{macros}

\begin{document}

\title{Resilient Distributed Datasets in Python \\ \small{6.824 Final Project -
Spring 2013}} \author{Eben Freeman \& Jonas Helfer\\
  \small{\{helfer,emfree\}@mit.edu} } \maketitle

\begin{abstract} Distributed computing framewokrs such as MapReduce
  are well-suited to certain tasks, but poorly suited to others. For example,
  iterative computations such as PageRank or many machine learning algorithms
  require running multiple MapReduce jobs, resulting in poor performance. The
  concept of resilient distributed datasets addresses these limitations by
  providing a way to persist data in memory across computations. We implemented
  a simple version of RDDs in Python.

\end{abstract} \section*{Introduction} Running multiple MapReduce jobs requires
reading to and writing from disk in between each job. This limits performance,
and makes live interaction with a dataset unnecessarily cumbersome. By caching data
in memory, we address both these limitations.
A resilient distributed dataset (RDD) is a distributed key-value set together
with a computation history that specifies how it was constructed.\cite{rdd} Transformations
such as map, reduce, filter, or join can be applied to yield a new RDD.
Users create RDDs by reading initial data from disk and specifying an
appropriate series of transformations. These can be evaluated lazily, with a
computation only executing when the program needs to yield data to the user.
Users can specify the partitioning of data across machines, or use a reasonable
default hash. RDD data is held in memory whenever possible, but can
be spilled to disk or replicated across machines as needed. In the event of
worker failure, lost data from an RDD can efficiently be reconstructed by using the RDD's
computation history to recompute the missing partitions.

We use a delay scheduling algorithm to schedule computations across worker
machines.\cite{delay} Delay scheduling gives preference to workers that already
have the requisite data in-memory, but provides flexibility to mitigate the
effect of stragglers.

\section*{Implementation} We implemented our system in Python. Our system is
lightweight and standalone, and does not require any particular additional
software such as HDFS. We first considered using Go to build on the existing
systems from labs 1 through 4, but we decided to use Python, because Python
makes serializing functions, a crucial aspect of RDDs, a lot easier.

Our system consists of one master and a variable number of workers that can be
added to and removed from the pool of workers. Workers can be added and removed
at any time.

Workers are RPC servers that spawn a new thread for every request and can thus
execute several requests concurrently. While the master will try to only send
one request at a time to any worker, the workers need to be able to send
requests for data to each other which need to be processed concurrently.

The scheduler runs on the master, which in general has only one thread. The
scheduler can be invoked from a program running on the master or interactively
by the user.  If the user passes an RDD to the scheduler, the scheduler will
try to add it to the dispatcher queue. If the RDD has no parents or if its
parents have already been computed, the partitions of the RDD are added to the
dispatcher queue. The jobs in the queue are assigned to workers according to
the delay scheduling scheme \cite{delay}, which balances data locality and
worker load. It is highly preferable to schedule a task on the worker that has
the input data in memory because fetching it from another worker will take time
and network bandwidth.

In principle, multiple users could interact concurrently with the same
scheduler without any problem.

\section*{Results} We tested our system using multiple threads communicating
over RPC on single dual-core machines. While that allowed us to simulate
race-conditions or deadlocks that might occur and thus debug our code, it did
not allow us to do a meaningful performance evaluation. However, since our
system is intended as a proof-of-concept and in no way optimized for
performance, we considered this drawback acceptable.

\section*{Conclusion} Our system is mainly a proof of concept.

\subsection*{Project experience} Challenging parts? Debugging through rpc.




\bibliographystyle{plain} \bibliography{6.824-report}

\end{document}
